# Data Engineering Challenge (Consumer)

Expectation - Obtain large amount of data and process based on given constraints. Sample file containing 100,000 github respositories were provided. Solution should include application that can run multiple nano/micro instances.

Solution - Microservices is appropriate for developing applications that can take adavantage of nano/micro instances. Given my proficiency in Spring Boot, it was used to develop the application. 

Architecture - The concept of producer/consumer was utilized. This module is the consumer and it takes advantage of many data architect frameworks and libraries written in Java to develop the solution. 

Spring Boot - foremost microservices Java framework which can be used to develop production grade applications

Apache Spark - is a unified analytics engine for large-scale data processing. Reading a file of 100,000 lines is a walk in the park for the framework, so it serves as a means of loading the data, dividing it into batches and forward to the consumer for processing of application logic.

Apache Kafka - is one of the most efficient streaming frameworks which enables messaging and queuing across and between microservices. 

The producer module (dataengineeringengine) loads the files into memory and has a controller endpoint which will be curl (ed) to trigger analysis. File can be uploaded too but for development purposes, there is a sample list taken from the 100,000 URLs csv file provided.

To run the consumer module, the producer should have been started to recieve files to process. Apache Kafta have to be installed on the system.  Alongside Zookeeper. Should be started when setting up the producer.

Clone the consumer module to your computer

Consumer
git clone https://github.com/tksilicon/dataengineering.git
cd  dataengineering

mvn install  
mvn spring-boot:run  

The above command will get the consumer ready to accept repository URLs to process. The processed file (results.json) as expected can be found at the root of the application in dataset folder.

Because this is heavy data processing, taking advantage of aws elastic beanstalk services, the application was deployed on beanstalk  for testing which has the capability to scale the application and create multiple instances and load balance them and share the workload.

To deploy on aws,  note that you have to change the port in application.properties to 5000 when deploying to aws. With a user account. Locate beanstalk and upload the jar files generated by mvn package or from root of application folder:


use ebs cli

eb init
eb create 

eb scale 3 

The above will deploy the modules. However, Apache Kafka is a paid service on aws so a free 30 day trial from CloudKafka service was used for testing. The CloudKafka service works while the appplication is working on local too. To run on CloudKafka service  comment the local configuration and uncomment CloudKafka. 



The Json outputs provides the deliverables
-Number of lines of code 
-List of external libraries/packages
-The Nesting factor for the repository
-Code duplication(not concluded)
-Average number of parameters 
-Average Number of variables

The solution leverages modern, recent and trending technologies to deliver
- Use of distributed systems, Efficiency, Accuracy, Optimisations, Comments as stipulated.


